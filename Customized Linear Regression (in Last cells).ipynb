{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.使用梯度下降求解\n",
    "$\\mathrm{L}=\\left(\\theta_{1}-3\\right)^{2}+\\left(2 \\theta_{2}-5\\right)^{2}$的最小值点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6000000000000001 2.0\n",
      "1.08 2.4\n",
      "1.464 2.48\n",
      "1.7711999999999999 2.496\n",
      "2.01696 2.4992\n",
      "2.213568 2.49984\n",
      "2.3708544 2.499968\n",
      "2.49668352 2.4999936\n",
      "2.597346816 2.49999872\n",
      "2.6778774528 2.499999744\n",
      "2.74230196224 2.4999999488\n",
      "2.793841569792 2.49999998976\n",
      "2.8350732558336 2.499999997952\n",
      "2.86805860466688 2.4999999995904\n",
      "2.894446883733504 2.49999999991808\n",
      "2.9155575069868034 2.499999999983616\n",
      "2.932446005589443 2.499999999996723\n",
      "2.945956804471554 2.4999999999993445\n",
      "2.9567654435772432 2.499999999999869\n",
      "2.9654123548617948 2.499999999999974\n",
      "2.9723298838894356 2.4999999999999947\n",
      "2.9778639071115487 2.499999999999999\n",
      "2.982291125689239 2.5\n",
      "2.985832900551391 2.5\n",
      "2.988666320441113 2.5\n",
      "2.9909330563528904 2.5\n",
      "2.9927464450823122 2.5\n",
      "2.99419715606585 2.5\n",
      "2.99535772485268 2.5\n",
      "2.996286179882144 2.5\n",
      "2.997028943905715 2.5\n",
      "2.9976231551245722 2.5\n",
      "2.9980985240996576 2.5\n",
      "2.9984788192797263 2.5\n",
      "2.998783055423781 2.5\n",
      "2.999026444339025 2.5\n",
      "2.99922115547122 2.5\n",
      "2.9993769243769757 2.5\n",
      "2.9995015395015807 2.5\n",
      "2.9996012316012646 2.5\n",
      "2.9996809852810116 2.5\n",
      "2.999744788224809 2.5\n",
      "2.9997958305798473 2.5\n",
      "2.999836664463878 2.5\n",
      "2.9998693315711025 2.5\n",
      "2.999895465256882 2.5\n",
      "2.9999163722055053 2.5\n",
      "2.999933097764404 2.5\n",
      "2.9999464782115233 2.5\n",
      "2.9999571825692186 2.5\n",
      "2.999965746055375 2.5\n",
      "2.9999725968443 2.5\n",
      "2.99997807747544 2.5\n",
      "2.999982461980352 2.5\n",
      "2.9999859695842814 2.5\n",
      "2.999988775667425 2.5\n",
      "2.99999102053394 2.5\n",
      "2.999992816427152 2.5\n",
      "2.9999942531417214 2.5\n",
      "2.999995402513377 2.5\n",
      "2.9999963220107015 2.5\n",
      "2.999997057608561 2.5\n",
      "2.999997646086849 2.5\n",
      "2.999998116869479 2.5\n",
      "2.9999984934955832 2.5\n",
      "2.9999987947964666 2.5\n",
      "2.999999035837173 2.5\n",
      "2.9999992286697386 2.5\n",
      "2.999999382935791 2.5\n",
      "2.9999995063486327 2.5\n",
      "2.999999605078906 2.5\n",
      "2.9999996840631247 2.5\n",
      "2.9999997472504996 2.5\n",
      "2.9999997978004 2.5\n",
      "2.9999998382403197 2.5\n",
      "2.999999870592256 2.5\n",
      "2.999999896473805 2.5\n",
      "2.9999999171790437 2.5\n",
      "2.999999933743235 2.5\n",
      "2.999999946994588 2.5\n",
      "2.99999995759567 2.5\n",
      "2.999999966076536 2.5\n",
      "2.9999999728612288 2.5\n",
      "2.999999978288983 2.5\n",
      "2.9999999826311865 2.5\n",
      "2.999999986104949 2.5\n",
      "2.9999999888839595 2.5\n",
      "2.9999999911071678 2.5\n",
      "2.9999999928857344 2.5\n",
      "2.9999999943085873 2.5\n",
      "2.9999999954468697 2.5\n",
      "2.999999996357496 2.5\n",
      "2.9999999970859967 2.5\n",
      "2.999999997668797 2.5\n",
      "2.9999999981350376 2.5\n",
      "2.99999999850803 2.5\n",
      "2.999999998806424 2.5\n",
      "2.9999999990451394 2.5\n",
      "2.9999999992361115 2.5\n",
      "2.9999999993888893 2.5\n"
     ]
    }
   ],
   "source": [
    "theta1 , theta2 = 0.0,0.0\n",
    "alpha = 0.1\n",
    "for i in range(100):\n",
    "    theta1 = theta1 - alpha*2*(theta1-3)\n",
    "    theta2 = theta2 - alpha*2*(2*theta2 -5)*2\n",
    "    print(theta1,theta2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 使用梯度下降求解\n",
    "$\\mathrm{L}=\\left(\\theta_{1}-3\\right)^{2}\\left(2 \\theta_{2}-5\\right)^{2}$的最小值点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.74 0.04628800000000011\n",
      "5.975293375748915 1.783981451662664\n",
      "5.85326289340646 2.2503162634613827\n",
      "5.839032651420127 2.411314462006146\n",
      "5.8372463037651885 2.4684277114229185\n",
      "5.837020048260971 2.4887569381894847\n",
      "5.836991358852866 2.49599613879014\n",
      "5.836987720489843 2.498574144730124\n",
      "5.836987259067804 2.4994922240457913\n",
      "5.836987200549514 2.499819170679976\n",
      "5.8369871931281265 2.499935603009519\n",
      "5.836987192186934 2.499977066924836\n",
      "5.83698719206757 2.499991833066536\n",
      "5.836987192052432 2.4999970915892553\n",
      "5.836987192050512 2.4999989642559113\n",
      "5.836987192050269 2.499999631150511\n",
      "5.836987192050238 2.4999998686452116\n",
      "5.836987192050234 2.499999953221894\n",
      "5.836987192050233 2.4999999833413673\n",
      "5.836987192050233 2.499999994067523\n",
      "5.836987192050233 2.4999999978873246\n",
      "5.836987192050233 2.4999999992476334\n",
      "5.836987192050233 2.4999999997320668\n",
      "5.836987192050233 2.4999999999045834\n",
      "5.836987192050233 2.49999999996602\n",
      "5.836987192050233 2.499999999987899\n",
      "5.836987192050233 2.4999999999956906\n",
      "5.836987192050233 2.4999999999984652\n",
      "5.836987192050233 2.4999999999994533\n",
      "5.836987192050233 2.4999999999998055\n",
      "5.836987192050233 2.4999999999999307\n",
      "5.836987192050233 2.499999999999975\n",
      "5.836987192050233 2.499999999999991\n",
      "5.836987192050233 2.499999999999997\n",
      "5.836987192050233 2.499999999999999\n",
      "5.836987192050233 2.4999999999999996\n",
      "5.836987192050233 2.5\n",
      "5.836987192050233 2.5\n",
      "5.836987192050233 2.5\n",
      "5.836987192050233 2.5\n",
      "5.836987192050233 2.5\n",
      "5.836987192050233 2.5\n",
      "5.836987192050233 2.5\n",
      "5.836987192050233 2.5\n",
      "5.836987192050233 2.5\n",
      "5.836987192050233 2.5\n",
      "5.836987192050233 2.5\n",
      "5.836987192050233 2.5\n",
      "5.836987192050233 2.5\n",
      "5.836987192050233 2.5\n",
      "5.836987192050233 2.5\n",
      "5.836987192050233 2.5\n",
      "5.836987192050233 2.5\n",
      "5.836987192050233 2.5\n",
      "5.836987192050233 2.5\n",
      "5.836987192050233 2.5\n",
      "5.836987192050233 2.5\n",
      "5.836987192050233 2.5\n",
      "5.836987192050233 2.5\n",
      "5.836987192050233 2.5\n",
      "5.836987192050233 2.5\n",
      "5.836987192050233 2.5\n",
      "5.836987192050233 2.5\n",
      "5.836987192050233 2.5\n",
      "5.836987192050233 2.5\n",
      "5.836987192050233 2.5\n",
      "5.836987192050233 2.5\n",
      "5.836987192050233 2.5\n",
      "5.836987192050233 2.5\n",
      "5.836987192050233 2.5\n",
      "5.836987192050233 2.5\n",
      "5.836987192050233 2.5\n",
      "5.836987192050233 2.5\n",
      "5.836987192050233 2.5\n",
      "5.836987192050233 2.5\n",
      "5.836987192050233 2.5\n",
      "5.836987192050233 2.5\n",
      "5.836987192050233 2.5\n",
      "5.836987192050233 2.5\n",
      "5.836987192050233 2.5\n",
      "5.836987192050233 2.5\n",
      "5.836987192050233 2.5\n",
      "5.836987192050233 2.5\n",
      "5.836987192050233 2.5\n",
      "5.836987192050233 2.5\n",
      "5.836987192050233 2.5\n",
      "5.836987192050233 2.5\n",
      "5.836987192050233 2.5\n",
      "5.836987192050233 2.5\n",
      "5.836987192050233 2.5\n",
      "5.836987192050233 2.5\n",
      "5.836987192050233 2.5\n",
      "5.836987192050233 2.5\n",
      "5.836987192050233 2.5\n",
      "5.836987192050233 2.5\n",
      "5.836987192050233 2.5\n",
      "5.836987192050233 2.5\n",
      "5.836987192050233 2.5\n",
      "5.836987192050233 2.5\n",
      "5.836987192050233 2.5\n"
     ]
    }
   ],
   "source": [
    "theta1 , theta2 = 10.0,4.0\n",
    "alpha = 0.01\n",
    "for i in range(100):\n",
    "    theta1 = theta1 - alpha*2*(theta1-3)*(2*theta2-5)*(2*theta2-5)\n",
    "    theta2 = theta2 - alpha*2*(2*theta2 -5)*2*(theta1-3)*(theta1-3)\n",
    "    print(theta1,theta2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 使用梯度下降实现最小二乘法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LinearRegression(X,y,alpha,n_rounds):\n",
    "    n_features = X.shape[1]\n",
    "    beta = np.array([0.0]*n_features)\n",
    "    for i in range(n_rounds):\n",
    "        \n",
    "        #计算 epsilon\n",
    "        epsilon = y\n",
    "        for j in range(n_features):\n",
    "            epsilon = epsilon - beta[j]*X[:,j]\n",
    "            \n",
    "        #更新 beta\n",
    "        for j in range(n_features):\n",
    "            gradient = -np.mean(epsilon*X[:,j])\n",
    "            beta[j] = beta[j] - alpha*gradient\n",
    "\n",
    "    return beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.24430801, 0.25516176, 0.10030543, 0.81107587])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 确认实现的LinearRegression函数得到的结果与sklearn中的LinearRegression的结果相同\n",
    "data = pd.read_csv('height_train.csv')\n",
    "data['constant'] = 1\n",
    "LinearRegression(data.loc[:,['father_height','mother_height','boy_dummy','constant']].values,data.child_height.values,0.1,100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 收敛条件的判断\n",
    "2. 学习率的选择\n",
    "3. 类的形式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLinearRegression:\n",
    "    \"\"\"\n",
    "    Linear Regression Using Mean Squred Error as Loss Function\n",
    "    \"\"\"\n",
    "    def __init__(self, epsilon=1e-6, lr_auto_adap_trials=10, lr_decay_rate=0.5):\n",
    "        \"\"\"\n",
    "        # epsilon: a small number for convergence check\n",
    "        # lr_auto_adap_trials: the max number of learning rate adaption (lr_new = lr_old * 0.5)\n",
    "        # lr_decay_rate: decay rate of learning rate for each auto adaption\n",
    "        \"\"\"\n",
    "        self.epsilon = epsilon\n",
    "        self.lr_auto_adap_trials = lr_auto_adap_trials\n",
    "        self.lr_decay_rate = lr_decay_rate\n",
    "        return\n",
    "        \n",
    "    def _fit(self, X, y):\n",
    "        for i in range(self.max_iter):\n",
    "            grads = np.dot(X, self.betas) + self.bias - y\n",
    "            loss = np.mean(grads**2)/2\n",
    "            neg_grad_bias = -np.mean(grads)\n",
    "            neg_grad_betas = -np.mean(grads * X , axis=0, keepdims=True).T\n",
    "            self.betas += self.lr * neg_grad_betas\n",
    "            self.bias += self.lr * neg_grad_bias\n",
    "            \n",
    "            # check convergence:\n",
    "            max_abs_change = max(np.max(np.abs(neg_grad_betas)), np.abs(neg_grad_bias))\n",
    "            if max_abs_change < self.epsilon:\n",
    "                return True\n",
    "        return False\n",
    "#             if (i+1) % 50 == 0:\n",
    "#                 print(\"Loss after {} iterration: {}\".format(i, loss))\n",
    "            \n",
    "    def print_model(self):\n",
    "        terms = [\"{:.4f}*X{}\".format(beta[0], i) for i, beta in enumerate(self.betas.tolist(), 1)]\n",
    "        terms += [\"{:.4f}\".format(self.bias)]\n",
    "        print(\"y = \" + \" + \".join(terms))\n",
    "        \n",
    "    def fit(self, X, y, lr=0.01, max_iter=100):\n",
    "        self.lr = lr\n",
    "        self.max_iter=max_iter\n",
    "        y = np.array(y).reshape(-1, 1)\n",
    "        for i in range(self.lr_auto_adap_trials):\n",
    "            self.betas = np.random.randn(np.shape(X)[1], 1)\n",
    "            self.bias = 1e-10\n",
    "            if self._fit(X, y):\n",
    "                print(\"Converged at lr {} (diff threshold: {})\".format(self.lr, self.epsilon))\n",
    "                break\n",
    "            else:\n",
    "                self.lr *= self.lr_decay_rate\n",
    "                print(\"Hasn't converged, decay learning rate and continue ({}/{} trial[s])\\nNew lr: {} (diff threshold: {})\".format(i+1, self.lr_auto_adap_trials, self.lr, self.epsilon))\n",
    "                \n",
    "        return self\n",
    "        \n",
    "    def predict(self, X):\n",
    "        return np.dot(X, self.betas) + self.bias\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chen/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:19: RuntimeWarning: overflow encountered in square\n",
      "/home/chen/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:21: RuntimeWarning: invalid value encountered in multiply\n",
      "/home/chen/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:22: RuntimeWarning: invalid value encountered in add\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hasn't converged, decay learning rate and continue (1/10 trial[s])\n",
      "New lr: 0.5 (diff threshold: 1e-06)\n",
      "Hasn't converged, decay learning rate and continue (2/10 trial[s])\n",
      "New lr: 0.25 (diff threshold: 1e-06)\n",
      "Converged at lr 0.25 (diff threshold: 1e-06)\n",
      "y = 0.2405*X1 + 0.2511*X2 + 0.1003*X3 + 0.6112*X4 + 0.2130\n"
     ]
    }
   ],
   "source": [
    "my_lr = MyLinearRegression()\n",
    "my_lr.fit(data.loc[:,['father_height','mother_height','boy_dummy','constant']].values, data.child_height.values,1.0,100000)\n",
    "my_lr.print_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.23959427 0.25013358 0.10030806 0.        ]\n",
      "0.8274299645517081\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression(n_jobs=-1)\n",
    "lr.fit(data.loc[:,['father_height','mother_height','boy_dummy','constant']].values, data.child_height.values)\n",
    "print(lr.coef_)\n",
    "print(lr.intercept_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
